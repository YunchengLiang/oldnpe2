# Databricks notebook source
from pyspark.sql.functions import *
from pyspark.sql.window import Window

# COMMAND ----------

# MAGIC %md
# MAGIC cancellations: 101000264, 101001648, 109000006
# MAGIC move requests: 109000011 
# MAGIC over all verint conversation

# COMMAND ----------

# MAGIC %md
# MAGIC when removing duplicates entries, if a customer has been transfered to different agent, the connection id will be the same, use (connection id, employee id) to remove duplicates, sometimes has same conversation context. If a customer is overall talking about one main topic, regardless if they are transferred, the category will be the same.    
# MAGIC     
# MAGIC 
# MAGIC Verint table:  (sumfct_conversation, session_booked)   
# MAGIC Speech_id_verint: is an internal ID that should be unique to each call     
# MAGIC for the entire call, but it's generated by the Azure system, connection and interaction are generated by verint     
# MAGIC it's used to connect verint tables within Azure     
# MAGIC interaction_id: is each segment of a call so each customer/agent interaction      
# MAGIC A segment is defined by each unique customer/agent interaction, so it gets a new id when they transfer      
# MAGIC connection_id: is for the entire call, so all the customer/agent interactions between when someone calls and hangs up     
# MAGIC so a connection id can contain many interaction ids, if the customer gets transferred a lot. but it could also have only one, if they call into a single agent and then hang up     
# MAGIC ctn: customer telephone number     
# MAGIC Customer_id: same as BAN, CAN     
# MAGIC Receiving_skill: specific plan code      
# MAGIC Record_insert_dt: refers to when it entered the verint system, different from the conversation_date     
# MAGIC  
# MAGIC 
# MAGIC Session_categories table:     
# MAGIC category_id is created by verint     
# MAGIC instance ID is a broader definition, so every call gets a instance, and then category, there can be many categories within an instance     
# MAGIC every entire call will be treated as an instance, and each call text will contain one or more categories depends on each customer     

# COMMAND ----------

# MAGIC %md
# MAGIC Verint table:  (sumfct_conversation, session_booked)    
# MAGIC Session_categories table:

# COMMAND ----------

import logging

logger = logging.getLogger("Household_DataOPs")
class Household:
    """
        This class wraps data operations for household
    """

    ### Table names & queries on Databricks ###
    # household_table = "APP_IBRO.IBRO_HOUSEHOLD_ACTIVITY"
    household_table= "ml_etl_output_data.IBRO_HOUSEHOLD_ACTIVITY_CHURN_SCHEDULED"
    nonchurn_table = "DEFAULT.IBRO_HOUSEHOLD_ACTIVITY_NONCHURN_SCHEDULED"

    @staticmethod
    def load_data(spark_s = None, churn = True):
        """
        Loading ibro_household_activity data to spark dataframes
        :param condition: Optional condition to be passed to household_query e.g. "where []"
        :return ibro_household_activity dataframes respectively
        """

        # those columns can be used to filter our churned customers
        # find out CAN, treated as account_number
        # ARPA_OUT = -1 means deac, ARPA_OUT = 1 means winback
        if churn:
            household_query = "select * from {0}".format(Household.household_table)

        if not churn:
            household_query = "select * from {0}".format(Household.nonchurn_table)

        logger.info("reading ibro_household_activity data from Databricks.")
        logger.debug("running queries: {0}\n".format(household_query))
        return spark_s.sql(household_query) 

# COMMAND ----------

household_df = Household.load_data(spark_s = spark, churn = False)
print("=======================household columns===============================")
print(household_df.columns)

# COMMAND ----------

# from rogers.cd.SparkOPs import DataBricks
import logging


logger = logging.getLogger("Verint_DataOPs")


class Verint:
    """
        This class wraps data operations for verint
    """

    ### Table names & queries on Databricks ###
    verint_table = "VERINT.CBU_ROG_CONVERSATION_SUMFCT"
    booked_table = "VERINT.SESSIONS_BOOKED"
    
    #91  filter version for categories and instances
    @staticmethod
    def load_data_filter(min_date = None, max_date = None, categories = [], instances = [], spark_s = None):
        """
        Loading cbu_rog_conversation_sumfct data to spark dataframes
        :param condition: Optional condition to be passed to verint_query e.g. "where []"
        :return cbu_rog_conversation_sumfct dataframes respectively
        """
        #91 add distinct for sid_key
        if instances != [] and categories != []:
            print('Categories and instances provided')
            #Category id can be used to filter our churned customers
            verint_query = "SELECT * FROM ( \
                (SELECT distinct speech_id FROM \
                (SELECT distinct sid_key FROM VERINT.SESSIONS_CATEGORIES WHERE category_id not in ('{0}') AND instance_id not in ('{1}')) as category \
                INNER JOIN \
                (SELECT CONCAT(unit_num, '0', channel_num) as speech_id, sid_key FROM {1}) as booked \
                ON booked.sid_key = category.sid_key) as merged \
                INNER JOIN \
                (SELECT distinct SPEECH_ID_VERINT,TEXT_AGENT_FULL, TEXT_CUSTOMER_FULL, TEXT_OVERLAP, TEXT_ALL,CUSTOMER_ID,CTN,INTERACTION_ID,AGENT_EMP_ID,CONNECTION_ID,RECEIVING_SKILL,LANGUAGE_INDICATOR, CONVERSATION_DATE, YEAR(conversation_date) as Year, MONTH(conversation_date) as Month FROM {3} WHERE conversation_date >= '{4}') \
                as sumfct ON sumfct.speech_id_verint = booked.speech_id) as final"\
                .format(("','".join(categories)),("','".join(instances)),Verint.booked_table,Verint.verint_table, min_date)
        #91 add distinct for sid_key
        elif instances == [] and categories != []:
            print('Categories provided')
            verint_query = "SELECT * FROM ( \
                (SELECT distinct speech_id FROM \
                (SELECT distinct sid_key FROM VERINT.SESSIONS_CATEGORIES WHERE category_id not in ('{0}')) as category \
                INNER JOIN \
                (SELECT CONCAT(unit_num, '0', channel_num) as speech_id, sid_key FROM {1}) as booked \
                ON booked.sid_key = category.sid_key) as merged \
                INNER JOIN \
                (SELECT distinct SPEECH_ID_VERINT,TEXT_AGENT_FULL, TEXT_CUSTOMER_FULL, TEXT_OVERLAP, TEXT_ALL,CUSTOMER_ID,CTN,INTERACTION_ID,AGENT_EMP_ID,CONNECTION_ID,RECEIVING_SKILL,LANGUAGE_INDICATOR, CONVERSATION_DATE,  YEAR(conversation_date) as Year, MONTH(conversation_date) as Month FROM {2} WHERE conversation_date >= '{3}') \
                as sumfct ON sumfct.speech_id_verint = merged.speech_id) as final" \
                .format(("','".join(categories)),Verint.booked_table,Verint.verint_table, min_date)
        #91 add distinct for sid_key
        elif instances != [] and categories == []:
            print('Instances provided')
            verint_query = "SELECT * FROM ( \
                (SELECT distinct speech_id FROM \
                (SELECT distinct sid_key FROM VERINT.SESSIONS_CATEGORIES WHERE instance_id not in ('{0}')) as category \
                INNER JOIN \
                (SELECT CONCAT(unit_num, '0', channel_num) as speech_id, sid_key FROM {1}) as booked \
                ON booked.sid_key = category.sid_key) as merged \
                INNER JOIN \
                (SELECT distinct SPEECH_ID_VERINT,TEXT_AGENT_FULL, TEXT_CUSTOMER_FULL, TEXT_OVERLAP, TEXT_ALL,CUSTOMER_ID,CTN,INTERACTION_ID,AGENT_EMP_ID,CONNECTION_ID,RECEIVING_SKILL,LANGUAGE_INDICATOR, CONVERSATION_DATE,  YEAR(conversation_date) as Year, MONTH(conversation_date) as Month FROM {2} WHERE conversation_date >= '{3}') \
                as sumfct ON sumfct.speech_id_verint = booked.speech_id"\
                .format(("','".join(instances)),Verint.booked_table,Verint.verint_table, min_date)
       
        else:
            print('Only date provided')
            verint_query = "SELECT distinct SPEECH_ID_VERINT,TEXT_AGENT_FULL, TEXT_CUSTOMER_FULL, TEXT_OVERLAP, TEXT_ALL,CUSTOMER_ID,CTN,INTERACTION_ID,AGENT_EMP_ID,CONNECTION_ID,RECEIVING_SKILL,LANGUAGE_INDICATOR, CONVERSATION_DATE, YEAR(conversation_date) as Year, MONTH(conversation_date) as Month FROM {1} WHERE conversation_date >= '{2}' " \
                .format(Verint.booked_table, Verint.verint_table, min_date)
        
        #consumer_vq_input = "dbfs:/FileStore/contacts_driver/consumer_vqs.parquet"

        #Payment Training
        #verint_query = "SELECT * FROM VERINT.CBU_ROG_CONVERSATION_SUMFCT as a INNER JOIN (SELECT LOWER(Conn_ID) as conn_id, L2 FROM ml_etl_output_data.verint_payments_sample WHERE L2 = 'Bill Inquiry') as b ON a.connection_id = b.conn_id"
        #Payment in Full Training
        #verint_query = "SELECT sumfct.* FROM (SELECT sid_key FROM VERINT.SESSIONS_CATEGORIES WHERE instance_id = '504111') as category INNER JOIN (SELECT CONCAT(unit_num, '0', channel_num) as speech_id, sid_key FROM VERINT.SESSIONS_BOOKED) as booked ON booked.sid_key = category.sid_key INNER JOIN VERINT.CBU_ROG_CONVERSATION_SUMFCT as sumfct ON sumfct.speech_id_verint = booked.speech_id INNER JOIN (SELECT LOWER(Conn_ID) as conn_id FROM ml_etl_output_data.verint_full_balance_sample) as sample ON sumfct.connection_id = sample.conn_id"
        #Competitive Intelligence Training
        #verint_query = "SELECT sumfct.* FROM (SELECT sid_key FROM VERINT.SESSIONS_CATEGORIES WHERE category_id in ('121000167', '125001150') AND instance_id in ('504121', '504125')) as category INNER JOIN (SELECT CONCAT(unit_num, '0', channel_num) as speech_id, sid_key FROM VERINT.SESSIONS_BOOKED) as booked ON booked.sid_key = category.sid_key INNER JOIN VERINT.CBU_ROG_CONVERSATION_SUMFCT as sumfct ON sumfct.speech_id_verint = booked.speech_id INNER JOIN (SELECT LOWER(ConnID) as conn_id FROM ml_etl_output_data.verint_competitive_mention_sample) as sample ON sumfct.connection_id = sample.conn_id"
        
        if max_date is not None:
            verint_query = "{0} where {1}".format(verint_query, "conversation_date < '{}'".format(max_date))
            
        
        # household table
        # household_query = "select * from {0}".format(Verint.household_table)

        logger.info("reading cbu_rog_conversation_sumfct data from Databricks.")
        logger.debug("running queries: {0}\n".format(verint_query))
        return spark_s.sql(verint_query) #, spark_s.sql(household_query)  #DataBricks.spark.read.format("parquet").load(consumer_vq_input)

# COMMAND ----------

spark_s=spark
verint_query=Verint.load_data_filter(min_date = '2022-05-01', max_date = '2022-05-09', categories = ['101000264', '101001648', '109000006','109000011'], instances = [], spark_s = spark)# 22 23 24 25 26 27 28 29 8days

# COMMAND ----------

#5-01 -> 5-09
print(verint_query.count())
print(verint_query.select("CONNECTION_ID","AGENT_EMP_ID").distinct().count())
print(verint_query.select("CONNECTION_ID").distinct().count())#352054 meaning 44006 call per day
print(verint_query.select("CONNECTION_ID","SPEECH_ID_VERINT").distinct().count())
#429555
#352054
#430316

# COMMAND ----------

non_null_verint_query=verint_query.dropna(subset=["CUSTOMER_ID"])
print(non_null_verint_query.count())
print(non_null_verint_query.select("CONNECTION_ID","AGENT_EMP_ID").distinct().count())
print(non_null_verint_query.select("CONNECTION_ID").distinct().count())#299458 meaning 37432 call per day for customers with id
print(non_null_verint_query.select("CONNECTION_ID","SPEECH_ID_VERINT").distinct().count())

# COMMAND ----------

verint=verint_query
ibro_verint_df = verint.join(household_df, verint.CUSTOMER_ID == household_df.HASH_LKP_ACCOUNT, 'inner')\
     .drop(household_df.CUSTOMER_ID)\
     .drop(household_df.CUSTOMER_COMPANY)\
     .drop(household_df.CUSTOMER_ACCOUNT)\
     .drop(household_df.REPORT_DATE)
print("=======================merged ibro and verint===============================")
print(ibro_verint_df.columns)
print("ibro_verint count rows: {}".format(ibro_verint_df.count()))
#上面这个应该会抱错? 因为join key不能有null 另外保险起见 应该只join需要的column 数据不稳定性太大 多一个column就多一个出现duplicates的可能性

# COMMAND ----------

display(ibro_verint_df)

# COMMAND ----------

# 2. merge before etl transformation 新版本
#一个很严肃的问题，一旦要求比如有customer_id 数据体量减少四分之一 430321->365288 等join上household table之后 数据体量再次大减到最初的四分之一 ->123702
verint=non_null_verint_query
household_df_new=household_df.select("HASH_LKP_ACCOUNT","HASH_LKP_ECID").distinct()
ibro_verint_df = verint.join(household_df_new, verint.CUSTOMER_ID == household_df_new.HASH_LKP_ACCOUNT, 'inner')
print("=======================merged ibro and verint===============================")
print(ibro_verint_df.columns)
print("ibro_verint count rows: {}".format(ibro_verint_df.count()))
#应该写成上面的样子，minimize systematic error

# COMMAND ----------

#看看ibro_verint_df里面是不是distinct customer count = distinct ecid
print(ibro_verint_df.select("CUSTOMER_ID").distinct().count())
print(ibro_verint_df.select("HASH_LKP_ECID").distinct().count())
#81874 != 79654 说明有些ecid对应了两个customer，一个企业多个customer？

# COMMAND ----------

household_df.createOrReplaceTempView("household_df_check")#一个customer_id应该对应一个ecid

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from household_df_check where HASH_LKP_ECID in (select HASH_LKP_ECID from household_df_check group by HASH_LKP_ECID having count(distinct HASH_LKP_ACCOUNT)>1 ) order by HASH_LKP_ECID

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from household_df_check where HASH_LKP_ACCOUNT in (select HASH_LKP_ACCOUNT from household_df_check group by HASH_LKP_ACCOUNT having count(distinct HASH_LKP_ECID)>1 ) order by HASH_LKP_ACCOUNT
# MAGIC # nice 说明只会有一个ecid对应多个customer 不会有一个customer对应多个ecid

# COMMAND ----------

# MAGIC %md
# MAGIC 对以下结果的解读: ecid总共3,709,383, 其中1,082,425个ecid对应多个customer（2299315）, 这些ecid平均一个对应两个customer

# COMMAND ----------

# MAGIC %sql
# MAGIC select count(distinct HASH_LKP_ECID) as rep_ecid_num_all from household_df_check 

# COMMAND ----------

# MAGIC %sql
# MAGIC select count(distinct HASH_LKP_ECID) as rep_ecid_num, count(distinct HASH_LKP_ACCOUNT) as corrsponding_customer_id_num from household_df_check where HASH_LKP_ECID in (select HASH_LKP_ECID from household_df_check group by HASH_LKP_ECID having count(distinct HASH_LKP_ACCOUNT)>1 ) 

# COMMAND ----------



# COMMAND ----------

#6-22 -> 6-30
print(verint_query.count())
print(verint_query.select("CONNECTION_ID","AGENT_EMP_ID").distinct().count())
print(verint_query.select("CONNECTION_ID").distinct().count())#352054 meaning 44006 call per day
print(verint_query.select("CONNECTION_ID","SPEECH_ID_VERINT").distinct().count())
#454906
#374431
#455567

# COMMAND ----------

display(verint_query)

# COMMAND ----------

print(verint_query.count())
#date range 5/01 - 5/08, 8 days, total 432073
# 429555 little bit smaller than 432073, worth investigation, but not main source of the problem
print(verint_query.select("CONNECTION_ID","AGENT_EMP_ID").distinct().count())
print(verint_query.select("CONNECTION_ID").distinct().count())#352054 meaning 44006 call per day
print(verint_query.select("CONNECTION_ID","SPEECH_ID_VERINT").distinct().count())
#430316<432073, could see duplicates but should not, meaning speech_id is not unique in this period, need investigation, but this problem could be explained by the fact that speech_veirnt_id is mistakenly desigined not to be unique and used twice for different call sometimes, that's a systematic error, talked to shaan before, nothing we could do, but i will try to find a way to fix it through downstream etl
#found out the error rises from sessions_booked, it has duplicates, even with small date range, different records appear under same speech_id

# COMMAND ----------

verint_query.createOrReplaceTempView("verint_5_01_v2")

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from verint_5_01_v2 where CONNECTION_ID in (select CONNECTION_ID from verint_5_01_v2 group by CONNECTION_ID, AGENT_EMP_ID having count(CONNECTION_ID)>1 ) order by CONNECTION_ID

# COMMAND ----------



# COMMAND ----------

verint_query.count()#455567 

# COMMAND ----------

verint_query.groupby(col("CONVERSATION_DATE")).count().show()

# COMMAND ----------

#date range 5/01 - 5/08, 8 days, total 432073
# 429555 little bit smaller than 432073, worth investigation, but not main source of the problem
print(verint_query.select("CONNECTION_ID","AGENT_EMP_ID").distinct().count())
print(verint_query.select("CONNECTION_ID").distinct().count())#352054 meaning 44006 call per day
print(verint_query.select("CONNECTION_ID","SPEECH_ID_VERINT").distinct().count())
#430316<432073, could see duplicates but should not, meaning speech_id is not unique in this period, need investigation, but this problem could be explained by the fact that speech_veirnt_id is mistakenly desigined not to be unique and used twice for different call sometimes, that's a systematic error, talked to shaan before, nothing we could do, but i will try to find a way to fix it through downstream etl
#found out the error rises from sessions_booked, it has duplicates, even with small date range, different records appear under same speech_id

# COMMAND ----------

verint_query.createOrReplaceTempView("verint_5_01")

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from verint_5_01 where SPEECH_ID_VERINT in (select SPEECH_ID_VERINT from verint_5_01 group by SPEECH_ID_VERINT having count(SPEECH_ID_VERINT)>1 ) order by SPEECH_ID_VERINT

# COMMAND ----------

#date range 6/22 - 6/29, 8 days
# 454906 little bit smaller than 455567, worth investigation, but not main source of the problem
print(verint_query.select("CONNECTION_ID","AGENT_EMP_ID").distinct().count())
print(verint_query.select("CONNECTION_ID").distinct().count())#374431 meaning 46803 call per day
print(verint_query.select("CONNECTION_ID","SPEECH_ID_VERINT").distinct().count())#make sense, since speech_id is unique

# COMMAND ----------



# COMMAND ----------

display(verint_query)#455,567 almost 57000 per day

# COMMAND ----------

verint_query.createOrReplaceTempView("verint_output")

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from verint_output where CONNECTION_ID in (select CONNECTION_ID from verint_output
# MAGIC group by CONNECTION_ID having count(CONNECTION_ID)>1) order by CONNECTION_ID

# COMMAND ----------

# MAGIC %md
# MAGIC below query has no result is good, meaning SPEECH_ID_VERINT is finally unqiue 

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from verint_output where SPEECH_ID_VERINT in (select SPEECH_ID_VERINT from verint_output
# MAGIC group by SPEECH_ID_VERINT having count(SPEECH_ID_VERINT)>1) 

# COMMAND ----------

print(verint_query.columns)
saved_columns=['SPEECH_ID_INTERNAL','SPEECH_ID_VERINT','TEXT_CUSTOMER_FULL', 'TEXT_ALL', 'CUSTOMER_ID', 'CTN',
                'INTERACTION_ID', 'CONNECTION_ID','RECEIVING_SKILL', 'CATEGORY_NAMES', 'LANGUAGE_INDICATOR',
               'CONVERSATION_DATE', 'Year', 'Month','session_duration', 'number_of_holds','hold_time_minutes',
 'account_number',
 'local_start_date',
 'local_start_time'
              ]

# COMMAND ----------



# COMMAND ----------

conv_sumfct=spark.sql("select * from verint.cbu_rog_conversation_sumfct")
session_booked=spark.sql("select * from verint.sessions_booked")
session_category=spark.sql("select * from verint.sessions_categories")

# COMMAND ----------

conv_sumfct.createOrReplaceTempView("conv_sum")

# COMMAND ----------

convresult=spark_s.sql("SELECT *, YEAR(conversation_date) as Year, MONTH(conversation_date) as Month FROM conv_sum")
print(convresult.count())#115968825
print(convresult.distinct().count())#115968825, no duplicates in conv_sumfct

# COMMAND ----------

convresult.select('SPEECH_ID_VERINT').distinct().count()
#85557759 investigate SPEECH_ID_VERINT's duplicates pattern, see is it connection_id
#is an internal ID that should be unique to each call for the entire call, 
#but it's generated by the Azure system, connection and interaction are generated by verint
#conv_sumfct's join column is SPEECH_ID_VERINT and it is not unique since each entire call could have many interactions
#what about the other table for join? the "speech_id"

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from conv_sum where SPEECH_ID_VERINT in (select SPEECH_ID_VERINT from conv_sum 
# MAGIC group by SPEECH_ID_VERINT having count(SPEECH_ID_VERINT)>1) 
# MAGIC '''CATEGORY_NAMES is the root of evil, just drop it'''

# COMMAND ----------

conv_sumfct_changed=conv_sumfct.drop('CATEGORY_NAMES','RECORD_INSERT_DT','SPEECH_ID_INTERNAL').distinct()
conv_sumfct_changed.createOrReplaceTempView("conv_sum_changed")

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from conv_sum_changed where SPEECH_ID_VERINT in (select SPEECH_ID_VERINT from conv_sum_changed 
# MAGIC group by SPEECH_ID_VERINT having count(SPEECH_ID_VERINT)>1) 
# MAGIC 
# MAGIC '''records are inserted multiple times, also a root of evil, suggest only select required columns from original query'''
# MAGIC '''SPEECH_ID_INTERNAL also a root of evil, two row with exactly same info but different SPEECH_ID_INTERNAL'''

# COMMAND ----------

# MAGIC %sql
# MAGIC select count(distinct SPEECH_ID_VERINT) from conv_sum_changed where SPEECH_ID_VERINT in (select SPEECH_ID_VERINT from conv_sum_changed 
# MAGIC group by SPEECH_ID_VERINT having count(SPEECH_ID_VERINT)>1) 
# MAGIC '''1944864 cases of same SPEECH_ID_VERINT but totally different content like customer,call'''

# COMMAND ----------

# MAGIC %sql
# MAGIC 
# MAGIC select count(distinct SPEECH_ID_VERINT) from conv_sum_changed where SPEECH_ID_VERINT in (select SPEECH_ID_VERINT from conv_sum_changed 
# MAGIC group by SPEECH_ID_VERINT,CONNECTION_ID having count(SPEECH_ID_VERINT)>1) 

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from conv_sum_changed where CONNECTION_ID in (select CONNECTION_ID from conv_sum_changed 
# MAGIC group by CONNECTION_ID having count(CONNECTION_ID)>1) 

# COMMAND ----------



# COMMAND ----------

conv_sumfct_changed.columns

# COMMAND ----------

display(convresult)

# COMMAND ----------

session_booked.createOrReplaceTempView("booked_session")

# COMMAND ----------

result=spark_s.sql("SELECT CONCAT(unit_num, '0', channel_num) as speech_id, sid_key, (sess_duration/60) as session_duration, number_of_holds, (total_hold_time/60) as hold_time_minutes, p7_value as account_number, CAST(local_start_time as DATE) as local_start_date, local_start_time FROM booked_session")
result.count()#35846940
result.distinct().count()#35846940  verint query part2 no duplicates

# COMMAND ----------

result.select("speech_id").distinct().count()#35674844 != 35846940 meaning speech_id here could also appear more than once

# COMMAND ----------

result.createOrReplaceTempView("resultlook")

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from resultlook where speech_id in (select speech_id from resultlook group by speech_id having count(speech_id)>1)
# MAGIC '''why same speech_id but different account_number'''
# MAGIC '''inner join between part 2 and 3 is not valid due to dupicates in join key on both side''

# COMMAND ----------

conv_sumfct.count()#115,622,019 too large

# COMMAND ----------

for i in ["CONNECTION_ID","CTN"]:
  print(i,' has count of ', conv_sumfct.select(i).distinct().count(),"\n")
#71,058,603 平均每次通话1.62个interaction(transfer)
#14,862,749 平均每个顾客4.8次通话

# COMMAND ----------

test_col=["SPEECH_ID_VERINT","TEXT_CUSTOMER_FULL","TEXT_ALL","CUSTOMER_ID","CTN","CONNECTION_ID","INTERACTION_ID","RECEIVING_SKILL","CATEGORY_NAMES",   "CONVERSATION_DATE"]
conv_sumfct_test=conv_sumfct.select(test_col).sample(0.01)

# COMMAND ----------

conv_sumfct_test.groupby("RECEIVING_SKILL").count().show()

# COMMAND ----------

#加feature 一次通话中被transfer的次数
transfer=conv_sumfct_test.groupby("CONNECTION_ID").count()
transfer=transfer.withColumnRenamed("CONNECTION_ID","CONNECTION_ID_2")
conv_sumfct_test_joined=conv_sumfct_test.join(transfer,conv_sumfct_test.CONNECTION_ID == transfer.CONNECTION_ID_2, 'inner')
conv_sumfct_test_joined=conv_sumfct_test_joined.withColumnRenamed("count","transfers_per_call").drop("CONNECTION_ID_2")
#没有CUSTOMER_ID或CTN或者TEXT_ALL的就drop掉吧 
conv_sumfct_test_joined=conv_sumfct_test_joined.dropna(subset=["TEXT_ALL","CUSTOMER_ID","CTN"])

# COMMAND ----------

conv_sumfct_test_joined.show()

# COMMAND ----------

conv_sumfct_test_joined.columns

# COMMAND ----------

#加feature 按照每个用户conversation recency给rank 1，2，3...
conv_sumfct_test_mid=conv_sumfct_test_joined.select(["CTN","CONNECTION_ID","CONVERSATION_DATE"]).dropDuplicates()#每个客户在不同日期的call,
windowSpec = Window.partitionBy("CTN").orderBy(desc("CONVERSATION_DATE"))#给每个人的call排recency
conv_sumfct_test_mid=conv_sumfct_test_mid.withColumn("call_recency",row_number().over(windowSpec))\
                      .select(["CTN","CONNECTION_ID","call_recency"])
conv_sumfct_test_mid=conv_sumfct_test_mid.withColumnRenamed("CTN","CTN2")
conv_sumfct_test_mid=conv_sumfct_test_mid.withColumnRenamed("CONNECTION_ID","CONNECTION_ID_2")

# COMMAND ----------

conv_sumfct_test_mid.show()

# COMMAND ----------

conv_sumfct_test_featured=conv_sumfct_test_joined.join(conv_sumfct_test_mid,(conv_sumfct_test_joined.CTN==conv_sumfct_test_mid.CTN2)&(conv_sumfct_test_joined.CONNECTION_ID==conv_sumfct_test_mid.CONNECTION_ID_2), "inner").drop("CTN2").drop("CONNECTION_ID_2")

# COMMAND ----------

conv_sumfct_test_featured.groupby("call_recency").count().show()

# COMMAND ----------

display(conv_sumfct_test_featured)

# COMMAND ----------

display(conv_sumfct_test_joined)

# COMMAND ----------

#session catergory data only keep cancel & move, which column could be used to join this data to coversation data
session_category_test_col=["SID_KEY","CATEGORY_ID","INSTANCE_ID"]
session_category_test=session_category.select(session_category_test_col).where(col("CATEGORY_ID").isin([101000264, 101001648, 109000006,109000011])).sample(0.01)

# COMMAND ----------

display(session_category_test)

# COMMAND ----------

display(conv_sumfct)

# COMMAND ----------

display(session_category)

# COMMAND ----------

display(session_booked)

# COMMAND ----------


