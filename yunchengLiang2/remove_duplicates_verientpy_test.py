# Databricks notebook source
from pyspark.sql.functions import *
from pyspark.sql.window import Window

# COMMAND ----------

# MAGIC %md
# MAGIC cancellations: 101000264, 101001648, 109000006
# MAGIC move requests: 109000011 
# MAGIC over all verint conversation

# COMMAND ----------

# MAGIC %md
# MAGIC when removing duplicates entries, if a customer has been transfered to different agent, the connection id will be the same, use (connection id, employee id) to remove duplicates, sometimes has same conversation context. If a customer is overall talking about one main topic, regardless if they are transferred, the category will be the same.    
# MAGIC     
# MAGIC 
# MAGIC Verint table:  (sumfct_conversation, session_booked)   
# MAGIC Speech_id_verint: is an internal ID that should be unique to each call     
# MAGIC for the entire call, but it's generated by the Azure system, connection and interaction are generated by verint     
# MAGIC it's used to connect verint tables within Azure     
# MAGIC interaction_id: is each segment of a call so each customer/agent interaction      
# MAGIC A segment is defined by each unique customer/agent interaction, so it gets a new id when they transfer      
# MAGIC connection_id: is for the entire call, so all the customer/agent interactions between when someone calls and hangs up     
# MAGIC so a connection id can contain many interaction ids, if the customer gets transferred a lot. but it could also have only one, if they call into a single agent and then hang up     
# MAGIC ctn: customer telephone number     
# MAGIC Customer_id: same as BAN, CAN     
# MAGIC Receiving_skill: specific plan code      
# MAGIC Record_insert_dt: refers to when it entered the verint system, different from the conversation_date     
# MAGIC  
# MAGIC 
# MAGIC Session_categories table:     
# MAGIC category_id is created by verint     
# MAGIC instance ID is a broader definition, so every call gets a instance, and then category, there can be many categories within an instance     
# MAGIC every entire call will be treated as an instance, and each call text will contain one or more categories depends on each customer     

# COMMAND ----------

# MAGIC %md
# MAGIC Verint table:  (sumfct_conversation, session_booked)    
# MAGIC Session_categories table:

# COMMAND ----------

import logging

logger = logging.getLogger("Household_DataOPs")
class Household:
    """
        This class wraps data operations for household
    """

    ### Table names & queries on Databricks ###
    # household_table = "APP_IBRO.IBRO_HOUSEHOLD_ACTIVITY"
    household_table= "ml_etl_output_data.IBRO_HOUSEHOLD_ACTIVITY_CHURN_SCHEDULED"
    nonchurn_table = "DEFAULT.IBRO_HOUSEHOLD_ACTIVITY_NONCHURN_SCHEDULED"

    @staticmethod
    def load_data(spark_s = None, churn = True):
        """
        Loading ibro_household_activity data to spark dataframes
        :param condition: Optional condition to be passed to household_query e.g. "where []"
        :return ibro_household_activity dataframes respectively
        """

        # those columns can be used to filter our churned customers
        # find out CAN, treated as account_number
        # ARPA_OUT = -1 means deac, ARPA_OUT = 1 means winback
        if churn:
            household_query = "select * from {0}".format(Household.household_table)

        if not churn:
            household_query = "select * from {0}".format(Household.nonchurn_table)

        logger.info("reading ibro_household_activity data from Databricks.")
        logger.debug("running queries: {0}\n".format(household_query))
        return spark_s.sql(household_query) 

# COMMAND ----------

household_df = Household.load_data(spark_s = spark, churn = False)
print("=======================household columns===============================")
print(household_df.columns)

# COMMAND ----------

# from rogers.cd.SparkOPs import DataBricks
import logging


logger = logging.getLogger("Verint_DataOPs")


class Verint:
    """
        This class wraps data operations for verint
    """

    ### Table names & queries on Databricks ###
    verint_table = "VERINT.CBU_ROG_CONVERSATION_SUMFCT"
    booked_table = "VERINT.SESSIONS_BOOKED"
    
    #91  filter version for categories and instances
    @staticmethod
    def load_data_filter(min_date = None, max_date = None, categories = [], instances = [], spark_s = None):
        """
        Loading cbu_rog_conversation_sumfct data to spark dataframes
        :param condition: Optional condition to be passed to verint_query e.g. "where []"
        :return cbu_rog_conversation_sumfct dataframes respectively
        """
        #91 add distinct for sid_key
        if instances != [] and categories != []:
            print('Categories and instances provided')
            #Category id can be used to filter our churned customers
            verint_query = "SELECT * FROM ( \
                (SELECT distinct speech_id FROM \
                (SELECT distinct sid_key FROM VERINT.SESSIONS_CATEGORIES WHERE category_id in ('{0}') AND instance_id in ('{1}')) as category \
                INNER JOIN \
                (SELECT CONCAT(unit_num, '0', channel_num) as speech_id, sid_key FROM {1}) as booked \
                ON booked.sid_key = category.sid_key) as merged \
                INNER JOIN \
                (SELECT distinct SPEECH_ID_VERINT,TEXT_AGENT_FULL, TEXT_CUSTOMER_FULL, TEXT_OVERLAP, TEXT_ALL,CUSTOMER_ID,CTN,INTERACTION_ID,AGENT_EMP_ID,CONNECTION_ID,RECEIVING_SKILL,LANGUAGE_INDICATOR, CONVERSATION_DATE, YEAR(conversation_date) as Year, MONTH(conversation_date) as Month FROM {3} WHERE conversation_date >= '{4}') \
                as sumfct ON sumfct.speech_id_verint = booked.speech_id) as final"\
                .format(("','".join(categories)),("','".join(instances)),Verint.booked_table,Verint.verint_table, min_date)
        #91 add distinct for sid_key
        elif instances == [] and categories != []:
            print('Categories provided')
            verint_query = "SELECT * FROM ( \
                (SELECT distinct speech_id FROM \
                (SELECT distinct sid_key FROM VERINT.SESSIONS_CATEGORIES WHERE category_id in ('{0}')) as category \
                INNER JOIN \
                (SELECT CONCAT(unit_num, '0', channel_num) as speech_id, sid_key FROM {1}) as booked \
                ON booked.sid_key = category.sid_key) as merged \
                INNER JOIN \
                (SELECT distinct SPEECH_ID_VERINT,TEXT_AGENT_FULL, TEXT_CUSTOMER_FULL, TEXT_OVERLAP, TEXT_ALL,CUSTOMER_ID,CTN,INTERACTION_ID,AGENT_EMP_ID,CONNECTION_ID,RECEIVING_SKILL,LANGUAGE_INDICATOR, CONVERSATION_DATE,  YEAR(conversation_date) as Year, MONTH(conversation_date) as Month FROM {2} WHERE conversation_date >= '{3}') \
                as sumfct ON sumfct.speech_id_verint = merged.speech_id) as final" \
                .format(("','".join(categories)),Verint.booked_table,Verint.verint_table, min_date)
        #91 add distinct for sid_key
        elif instances != [] and categories == []:
            print('Instances provided')
            verint_query = "SELECT * FROM ( \
                (SELECT distinct speech_id FROM \
                (SELECT distinct sid_key FROM VERINT.SESSIONS_CATEGORIES WHERE instance_id in ('{0}')) as category \
                INNER JOIN \
                (SELECT CONCAT(unit_num, '0', channel_num) as speech_id, sid_key FROM {1}) as booked \
                ON booked.sid_key = category.sid_key) as merged \
                INNER JOIN \
                (SELECT distinct SPEECH_ID_VERINT,TEXT_AGENT_FULL, TEXT_CUSTOMER_FULL, TEXT_OVERLAP, TEXT_ALL,CUSTOMER_ID,CTN,INTERACTION_ID,AGENT_EMP_ID,CONNECTION_ID,RECEIVING_SKILL,LANGUAGE_INDICATOR, CONVERSATION_DATE,  YEAR(conversation_date) as Year, MONTH(conversation_date) as Month FROM {2} WHERE conversation_date >= '{3}') \
                as sumfct ON sumfct.speech_id_verint = booked.speech_id"\
                .format(("','".join(instances)),Verint.booked_table,Verint.verint_table, min_date)
       
        else:
            print('Only date provided')
            verint_query = "SELECT distinct SPEECH_ID_VERINT,TEXT_AGENT_FULL, TEXT_CUSTOMER_FULL, TEXT_OVERLAP, TEXT_ALL,CUSTOMER_ID,CTN,INTERACTION_ID,AGENT_EMP_ID,CONNECTION_ID,RECEIVING_SKILL,LANGUAGE_INDICATOR, CONVERSATION_DATE, YEAR(conversation_date) as Year, MONTH(conversation_date) as Month FROM {1} WHERE conversation_date >= '{2}' " \
                .format(Verint.booked_table, Verint.verint_table, min_date)
        
        #consumer_vq_input = "dbfs:/FileStore/contacts_driver/consumer_vqs.parquet"

        #Payment Training
        #verint_query = "SELECT * FROM VERINT.CBU_ROG_CONVERSATION_SUMFCT as a INNER JOIN (SELECT LOWER(Conn_ID) as conn_id, L2 FROM ml_etl_output_data.verint_payments_sample WHERE L2 = 'Bill Inquiry') as b ON a.connection_id = b.conn_id"
        #Payment in Full Training
        #verint_query = "SELECT sumfct.* FROM (SELECT sid_key FROM VERINT.SESSIONS_CATEGORIES WHERE instance_id = '504111') as category INNER JOIN (SELECT CONCAT(unit_num, '0', channel_num) as speech_id, sid_key FROM VERINT.SESSIONS_BOOKED) as booked ON booked.sid_key = category.sid_key INNER JOIN VERINT.CBU_ROG_CONVERSATION_SUMFCT as sumfct ON sumfct.speech_id_verint = booked.speech_id INNER JOIN (SELECT LOWER(Conn_ID) as conn_id FROM ml_etl_output_data.verint_full_balance_sample) as sample ON sumfct.connection_id = sample.conn_id"
        #Competitive Intelligence Training
        #verint_query = "SELECT sumfct.* FROM (SELECT sid_key FROM VERINT.SESSIONS_CATEGORIES WHERE category_id in ('121000167', '125001150') AND instance_id in ('504121', '504125')) as category INNER JOIN (SELECT CONCAT(unit_num, '0', channel_num) as speech_id, sid_key FROM VERINT.SESSIONS_BOOKED) as booked ON booked.sid_key = category.sid_key INNER JOIN VERINT.CBU_ROG_CONVERSATION_SUMFCT as sumfct ON sumfct.speech_id_verint = booked.speech_id INNER JOIN (SELECT LOWER(ConnID) as conn_id FROM ml_etl_output_data.verint_competitive_mention_sample) as sample ON sumfct.connection_id = sample.conn_id"
        
        if max_date is not None:
            verint_query = "{0} where {1}".format(verint_query, "conversation_date < '{}'".format(max_date))
            
        
        # household table
        # household_query = "select * from {0}".format(Verint.household_table)

        logger.info("reading cbu_rog_conversation_sumfct data from Databricks.")
        logger.debug("running queries: {0}\n".format(verint_query))
        return spark_s.sql(verint_query) #, spark_s.sql(household_query)  #DataBricks.spark.read.format("parquet").load(consumer_vq_input)

# COMMAND ----------

spark_s=spark
verint_query=Verint.load_data_filter(min_date = '2022-05-01', max_date = '2022-05-09', categories = ['101000264', '101001648', '109000006','109000011'], instances = [], spark_s = spark)# 22 23 24 25 26 27 28 29 8days

# COMMAND ----------

#5-01 -> 5-09
print(verint_query.count())
print(verint_query.select("CONNECTION_ID","AGENT_EMP_ID").distinct().count())
print(verint_query.select("CONNECTION_ID").distinct().count())#352054 meaning 44006 call per day
print(verint_query.select("CONNECTION_ID","SPEECH_ID_VERINT").distinct().count())
#429555
#352054
#430316

# COMMAND ----------

non_null_verint_query=verint_query.dropna(subset=["CUSTOMER_ID"])
print(non_null_verint_query.count())
print(non_null_verint_query.select("CONNECTION_ID","AGENT_EMP_ID").distinct().count())
print(non_null_verint_query.select("CONNECTION_ID").distinct().count())#299458 meaning 37432 call per day for customers with id
print(non_null_verint_query.select("CONNECTION_ID","SPEECH_ID_VERINT").distinct().count())

# COMMAND ----------

verint=verint_query
ibro_verint_df = verint.join(household_df, verint.CUSTOMER_ID == household_df.HASH_LKP_ACCOUNT, 'inner')\
     .drop(household_df.CUSTOMER_ID)\
     .drop(household_df.CUSTOMER_COMPANY)\
     .drop(household_df.CUSTOMER_ACCOUNT)\
     .drop(household_df.REPORT_DATE)
print("=======================merged ibro and verint===============================")
print(ibro_verint_df.columns)
print("ibro_verint count rows: {}".format(ibro_verint_df.count()))
#上面这个应该会抱错? 因为join key不能有null 另外保险起见 应该只join需要的column 数据不稳定性太大 多一个column就多一个出现duplicates的可能性

# COMMAND ----------

display(ibro_verint_df)

# COMMAND ----------

# 2. merge before etl transformation 新版本
#一个很严肃的问题，一旦要求比如有customer_id 数据体量减少四分之一 430321->365288 等join上household table之后 数据体量再次大减到最初的四分之一 ->123702
verint=non_null_verint_query
household_df_new=household_df.select("HASH_LKP_ACCOUNT","HASH_LKP_ECID").distinct()
ibro_verint_df = verint.join(household_df_new, verint.CUSTOMER_ID == household_df_new.HASH_LKP_ACCOUNT, 'inner')
print("=======================merged ibro and verint===============================")
print(ibro_verint_df.columns)
print("ibro_verint count rows: {}".format(ibro_verint_df.count()))
#应该写成上面的样子，minimize systematic error

# COMMAND ----------

#看看ibro_verint_df里面是不是distinct customer count = distinct ecid
print(ibro_verint_df.select("CUSTOMER_ID").distinct().count())
print(ibro_verint_df.select("HASH_LKP_ECID").distinct().count())
#81874 != 79654 说明有些ecid对应了两个customer，一个企业多个customer？

# COMMAND ----------

household_df.createOrReplaceTempView("household_df_check")#一个customer_id应该对应一个ecid

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from household_df_check where HASH_LKP_ECID in (select HASH_LKP_ECID from household_df_check group by HASH_LKP_ECID having count(distinct HASH_LKP_ACCOUNT)>1 ) order by HASH_LKP_ECID

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from household_df_check where HASH_LKP_ACCOUNT in (select HASH_LKP_ACCOUNT from household_df_check group by HASH_LKP_ACCOUNT having count(distinct HASH_LKP_ECID)>1 ) order by HASH_LKP_ACCOUNT
# MAGIC # nice 说明只会有一个ecid对应多个customer 不会有一个customer对应多个ecid

# COMMAND ----------

# MAGIC %md
# MAGIC 对以下结果的解读: ecid总共3,709,383, 其中1,082,425个ecid对应多个customer（2299315）, 这些ecid平均一个对应两个customer

# COMMAND ----------

# MAGIC %sql
# MAGIC select count(distinct HASH_LKP_ECID) as rep_ecid_num_all from household_df_check 

# COMMAND ----------

# MAGIC %sql
# MAGIC select count(distinct HASH_LKP_ECID) as rep_ecid_num, count(distinct HASH_LKP_ACCOUNT) as corrsponding_customer_id_num from household_df_check where HASH_LKP_ECID in (select HASH_LKP_ECID from household_df_check group by HASH_LKP_ECID having count(distinct HASH_LKP_ACCOUNT)>1 ) 

# COMMAND ----------



# COMMAND ----------

#6-22 -> 6-30
print(verint_query.count())
print(verint_query.select("CONNECTION_ID","AGENT_EMP_ID").distinct().count())
print(verint_query.select("CONNECTION_ID").distinct().count())#352054 meaning 44006 call per day
print(verint_query.select("CONNECTION_ID","SPEECH_ID_VERINT").distinct().count())
#454906
#374431
#455567

# COMMAND ----------

display(verint_query)

# COMMAND ----------

print(verint_query.count())
#date range 5/01 - 5/08, 8 days, total 432073
# 429555 little bit smaller than 432073, worth investigation, but not main source of the problem
print(verint_query.select("CONNECTION_ID","AGENT_EMP_ID").distinct().count())
print(verint_query.select("CONNECTION_ID").distinct().count())#352054 meaning 44006 call per day
print(verint_query.select("CONNECTION_ID","SPEECH_ID_VERINT").distinct().count())
#430316<432073, could see duplicates but should not, meaning speech_id is not unique in this period, need investigation, but this problem could be explained by the fact that speech_veirnt_id is mistakenly desigined not to be unique and used twice for different call sometimes, that's a systematic error, talked to shaan before, nothing we could do, but i will try to find a way to fix it through downstream etl
#found out the error rises from sessions_booked, it has duplicates, even with small date range, different records appear under same speech_id

# COMMAND ----------

verint_query.createOrReplaceTempView("verint_5_01_v2")

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from verint_5_01_v2 where CONNECTION_ID in (select CONNECTION_ID from verint_5_01_v2 group by CONNECTION_ID, AGENT_EMP_ID having count(CONNECTION_ID)>1 ) order by CONNECTION_ID

# COMMAND ----------



# COMMAND ----------

verint_query.count()#455567 

# COMMAND ----------

verint_query.groupby(col("CONVERSATION_DATE")).count().show()

# COMMAND ----------

#date range 5/01 - 5/08, 8 days, total 432073
# 429555 little bit smaller than 432073, worth investigation, but not main source of the problem
print(verint_query.select("CONNECTION_ID","AGENT_EMP_ID").distinct().count())
print(verint_query.select("CONNECTION_ID").distinct().count())#352054 meaning 44006 call per day
print(verint_query.select("CONNECTION_ID","SPEECH_ID_VERINT").distinct().count())
#430316<432073, could see duplicates but should not, meaning speech_id is not unique in this period, need investigation, but this problem could be explained by the fact that speech_veirnt_id is mistakenly desigined not to be unique and used twice for different call sometimes, that's a systematic error, talked to shaan before, nothing we could do, but i will try to find a way to fix it through downstream etl
#found out the error rises from sessions_booked, it has duplicates, even with small date range, different records appear under same speech_id

# COMMAND ----------

verint_query.createOrReplaceTempView("verint_5_01")

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from verint_5_01 where SPEECH_ID_VERINT in (select SPEECH_ID_VERINT from verint_5_01 group by SPEECH_ID_VERINT having count(SPEECH_ID_VERINT)>1 ) order by SPEECH_ID_VERINT
